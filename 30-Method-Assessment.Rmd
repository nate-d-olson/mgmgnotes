## 16S rRNA Analysis Method Assessment
Method steps  
1. Sample to Seq
1. Seq to OTU  
2. Downstream analysis
    3. Taxonomic Classification  
    4. Differential Abundance  
    5. Ecological Diversity

## Sample to fastq
* Benchmarking  
    - using _in vitro_ mock communities (e.g. cells, DNA, or PCR products)  
    - technical replicates?  
    - well characterized benchmarking samples - similar to Genomic RMs (deep sequenced multiple methods)  
    - Mixture/ titrations  
    - Standard additions  
    - Dilution to extinction?  

## Seq to OTU  
### Pre-processing  
Schloss, P. D. (2010). The Effects of Alignment Quality, Distance Calculation Method, Sequence Filtering, and Region on the Analysis of 16S rRNA Gene-Based Studies. PLoS Comput Biol, 6(7), e1000844. http://doi.org/10.1371/journal.pcbi.1000844  

Albanese, D., Fontana, P., De Filippo, C., Cavalieri, D., & Donati, C. (2015). MICCA: a complete and accurate software for taxonomic profiling of metagenomic data. Scientific Reports, 5, 9743. http://doi.org/10.1038/srep09743  
* Evaluates preprocessing % read passing filtering, chimeras, and redundant OTUs, relative abundance  

Gaspar, J. M., & Thomas, W. K. (2013). Assessing the consequences of denoising marker-based metagenomic data. PloS One, 8(3), e60458. http://doi.org/10.1371/journal.pone.0060458  
* Evaluation of denoising and chimera detection methods for 454 data

### Chimera Detection  
Mysara, M., Saeys, Y., Leys, N., Raes, J., & Monsieurs, P. (2015). CATCh, an Ensemble Classifier for Chimera Detection in 16S rRNA Sequencing Studies. Applied and Environmental Microbiology, 81(December), 1573–1584. http://doi.org/10.1128/AEM.02896-14  

Beblur - https://github.com/ekopylova/deblur  

### OTU Assignment
* De novo clustering has been shown to be more accurate [@Westcott2015]  
* Types of datasets used to assess benchmarking methods: simulated data, mock communities, environmental datasests
* Approaches to evaluating clustering methods  
    - Cluster stability - similarity in clustering results when varying input data  
    - Cluster reproducibility - similarity in clustering results between methods  
    - Number of artifact OTUs - number of observed OTUs relative to expected   
        + Evaluating clustering methods using simulated and mock community clustering data provide a level of truth (expectation of the correct result).   

#### Summary of Cluster Method Assessment Papers

* @Kopylova2014 used a combination of simulated, mock communities, and data from environmental samples to compare clustering methods.  
    - They evaluated the performance of the clustering methods using F-measure and phylogenetic distance.  
    - For environment datasets the clustering methods were evaluated based on the disimilarity of samples for a dataset based on the sum of the squared deviation for UniFrac PCoA (Procrustes M^2) and similarity to UCLUST (Pearson's correlation). __NOTE__ Still note sure what they are evaluating for Procrustes.  
    - The authors excluded singletons from there analysis, while this does help to eliminate spurious OTUs, the method may also exclude rare taxa.  
* @Schloss2016 argues that using simulated and mock communities is not representative of real environmental samples. The authors additionally, argue the approach used by @Kopylova2014 to evaluating clustering methods confounds the impact of read filtering with clustering methods. 
    - The author's evaluated environmental datasets using Mathew's correlation coefficient, for a truth table (TP, TN, FP, FN) based on distance between sequences in the same and different clusters. 
        + This method for evaluating clusters was first used in [@Schloss2011]  
    - Average neighbor performed the best based the MCC based assessment. However, this approach to evaluation is biased towards this method as the results are strictly assessed based on the sequence distance criteria.  Other clustering methods that attempt to address more nuianced issues with clustering, sequencing error and variability in sequence diversity allow for differences in distances within and between clusters.  
    - This approach to cluster evaluation is particularly suited for algorithms based on analytical proofs.
    - For example the DNACluster clustering algorithm employs a heuristic based on a proof that for any given cluster the distance between two sequences in the cluster is less than the defined threshold value and no sequence is closer to another cluster center than the center of the cluster it is assigned to [@Ghodsi2011].
        + @Ghodsi2011 used a similar approach to validate their clustering method, though only focusing on the within cluster pairwise distance.
* Cluster Robustness is another attribute for evaluating clustering methods [@He2015].
    - Cluster robustness is the reproduciblity of the OTU table generated by a clustering algorith.  
    - Cluster robustness was evaluated based on:
        + Unstable sequences - sequences represented by different centroids
        + Unstable OTUs - OTUs where membership was impacted by the number of sequences in a dataset.
        + using Mathew's correlation coefficient
            * Truth Table definitions
                - TP if two sequences clustered __together__ for the full and subsampled dataset  
                - TN if two sequences clustered __separately__ for the full and subsampled  
                - FN two sequences clustered __together__ in full but not subsampled.  
                - FP two sequences clustered __separately__ in full but together in subsampled dataset.
            * The MCC method uses the full dataset at the true value to benchmark the subsampled dataset against.  
            * __NOTE__ Is this a valid assumption for clustering?  
        + Impact of cluster stablity on Rarefaction curves, PCoA with Bray-Curtis and UniFrac.  
        + Tested for significance using ADNOIS, a non-parametric test measuring effect size, the amount of the observed variance explained by metadata variables. __NEED REF__ - See vegan manual for potenial references.
    - The resulting OTU table is dependent the number of sequences clustered.  
    - For some methods the OTU table is dependent on the order in which the sequences are provided [@Ghodsi2011].  
    - Closed reference clustering was the only method the produced stable clusters.  
    - This method discards sequences that do not cluster with reference sequences.  
    - Open-reference cluster, which performs _de novo_ clustering of unmatched sequences provides a more robust alternative clustering method to _de novo_ clustering while allowing for the diversity of novel OTUs.  
* Cluster method robustness can also be calculated using cross validation[@Chen2013].  
    - @Chen2013 subsampled the data to 90% five times and compared the clusters between replicates.  
    - Methods were evaluated using precision, recall, and NID  
        + precision - measure of cluster homogeneity  
        + recall - measure of cluster completeness  
        + NID - assess cluster globally???  
    - See assessment of cluster quality for metric definitions  
* Cluster robustness can also be evaluated based on threshold and 16S region [@Schmidt2015].  
    - The authors focus their assessment more on the comparability of results among different methods and note the importances of reproducibility.  
    - Quote from paper “how robust are biological findings to the choice of clustering method? We found that OTU demarcation may indeed be replicable: different methods provided (almost) identical partitions when twice clustering the exact same sets of sequences, but in randomized order (Fig. 4, diagonals). However, trends in reproducibility were less clear.”  
* Cluster method assessment based on OTU inflation.  
    - Clustering methods have also been evaluated based on the number of predicted OTUs relative to the number of expected OTUs [@Huse2010, @Kopylova2014].  
    - Here either simulated or data from a mock community, where the number of OTUs is based on the number of strains or genomes used to generate the assessment dataset.  
    - The limitation to this approach is that it does not account for contaminants when __in vitro__ mock communities are used, and neither __in silico__ or __in vitro__ datasets represent the true complexity and diversity of environmental samples.  


## Downstream analysis  
### taxonomic classification  
### differential abundance  
### ecological diversity  
