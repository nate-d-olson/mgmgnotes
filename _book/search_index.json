[
["index.html", "Measurement Science of Metagenomics 1 Metrology of Metagenomics 1.1 Microbiome 1.2 Measuring Microbiomes 1.3 Metrology", " Measurement Science of Metagenomics   1 Metrology of Metagenomics  1.1 Microbiome   1.2 Measuring Microbiomes  whole metagenomic sequencing  marker-gene metagenomics  16S vs. WMS    1.3 Metrology  No not meteorlogy  Biological variability and measurement error  Measurement Error  Systematic - results in systematic bias  Random - increases uncertainty in results but provides an accurate measure of the true mean given sufficient sample size   Biological Noise  Sources of variability due to biological complexity of the system e.g. difference in results based on 16S rRNA gene region    replicate, repeat, and reproduce  SI redefining the kilogram example???  Uncertainty budgets  top-down vs. bottom-up  analytical chemsitry text   Orthogonal methods  Genome in a bottle example     most robust method       - Benchmarking           + Observed results vs. Truth               * Dataset with known true value - can be obtained using                   - gold standard method                   - mock communities: in vitro or in silico                        + limitations and assumptions                   - Mixtures                     + Provides a degree of known truth               * Performance metrics                   - Quantitative                       + Variability - standard deviation/ variance                       + Bias - mean squared error??? (true value)           + Observed results vs. assumptions               * No general dataset requirements               * Performance metrics                   - evaluate results based on method assumption, e.g. Schloss clustering distance       - Robustness Testing           + Used to evaluate impact of different methods on results           + Comparison of orthogonal methods               * For fundamentally different methods (different assumptions and heuristics)                   - Can evaluate precision (variability), not necessarily accuracy but less susceptible to bias than is methods are based on the same assumptions or heuristics           + Used to evaluate how different steps in the process affect the results    ## Microbiome Mixture Benchmarking Dataset * Mixture Study    ## Robustness Testing Ecological Diversity  * Evaluating alpha and beta diversity metrics   * Factors       - preprocessing methods       - data characteristics           + sequencing error rates           + primer region           + read length   * OTU table       - depth/ coverage - sparsity       - when does the diversity metric break?    ## Guideance on developing an assessment framework * Outline measurement process - fishbone diagram   * Differentiate between biological variability and measurement error       - How are these two sources of variability currently addressed in pipelines       - Are there any known sources of biological variability not currently being addressed   * What is known about each source of variability?       - uncertainty budget?       - dark uncertianty - unaccounted for variability?   * Robustness testing       - determine largest source of bias/ uncertianty in measurement process   * Charaterize individual measurement process steps based on robustness testing/ sensitivity analysis       - benchmarking datasets           + environmental           + in lab prepared material - combination of known samples           + in lab prepared material - combination of unknown samples           + in silico - various levels of simulation based on reference and environmental data availablility       - for each type of dataset summarize benefits and limitations to each    ## Proposed Gold Standard Analysis Method * Trim primers - dereplicate   * Chimera check? - multiple methods orthogonal methods?, annotate don't filter   * BLAST 100% - all perfect matches   * Unmatched       - Multiple sequence alignment           + Filter sequences with low alignment scores       - phylogenetic placement   * Generate OTUs for different branch distances   * Keep unique singletons - based on distance from non-singleton clusters, vary similarity based on cluster size, likelihood model for whether sequencing error or biological mutation.       - Rare orgs highly similar to large OTU less likely ecologically different?   * Alternative taxa classification       - Multiple methods           + pairwise alignment           + kmer           + phylogenetic placement           + other...       - Multiple reference databases           + evaluate individual sequences based on confidence - presence in multiple databases, infernal and rnammer alignment score?               * supporting sequence data - whole genome, sanger, 16S PCR from isolate               * likelihood of being a chimera           + Compare other methods to results generated using gold standard pipeline               * performance metrics   * Copy number correction       - based on phylogenetic placement -->   "],
["s-rrna-gene.html", "2 16S rRNA Gene 2.1 General background about 16S rRNA Gene 2.2 16S rRNA phylogenetics 2.3 16S rRNA Databases", " 2 16S rRNA Gene Not sure where this fits in…  2.1 General background about 16S rRNA Gene  Biological function  Variable and conserved regions  Multicopy  Horizontal gene transfer  Gene diversity  secondary structure  Use of different variable regions     2.2 16S rRNA phylogenetics  Gene evolution  Phylogeny    2.3 16S rRNA Databases  RDP  Greengenes  NCBI  SILVA    "],
["process-characterization.html", "3 Process Characterization", " 3 Process Characterization  Tools  Validation  Fit for purpose - comparison to gold standard method or reference material  no gold standard methods - only community accepted defactor standards    Benchmarking  Comparison to known truth  Evaluating results based on expectation   Robustness testing  Impact of procedure or step on results    Characterization of Data Analysis Processing  Wet lab vs. dry lab  Validation  Currently not applicable … -&gt; most robust method   Benchmarking  Observed results vs. Truth  Dataset with known true value - can be obtained using  gold standard method  mock communities: in vitro or in silico  limitations and assumptions   Mixtures  Provides a degree of known truth    Performance metrics  Quantitative  Variability - standard deviation/ variance  Bias - mean squared error??? (true value)     Observed results vs. assumptions  No general dataset requirements  Performance metrics  evaluate results based on method assumption, e.g. Schloss clustering distance     Robustness Testing  Used to evaluate impact of different methods on results  Comparison of orthogonal methods  For fundamentally different methods (different assumptions and heuristics)  Can evaluate precision (variability), not necessarily accuracy but less susceptible to bias than is methods are based on the same assumptions or heuristics    Used to evaluate how different steps in the process affect the results     "],
["measurement-process.html", "4 Measurement Process 4.1 Sample to Seq 4.2 Seq to OTU 4.3 Downstream analysis 4.4 16S rRNA Analysis Method Assessment 4.5 Sample to fastq 4.6 Seq to OTU 4.7 Downstream analysis", " 4 Measurement Process Overview of the 16S rRNA marker gene metagenomics measurement process including the sources of error and bias introduced at different step in the process along with different mitigation strategies that have been developed to address each of the issues. Diagram of measurement process 4.1 .     Figure 4.1: Fishbone diagram of the 16S rRNA metagenomic sequencing measurement process.   Review of what is currently known about the sources of error and bias in the 16S metagenomics measurement process. Cause-effect diagrams are commonly used in process quality control to show the relationship between steps in the measurement process and sources of error or bias 4.2. Potentially may want to revise to focus on sources of error, e.g. cell type bias, preferential amplification, sequence errors, chimeras .    Figure 4.2: Causes (branch labels) of bias and variability indicated for different sources of error (branch text).    4.1 Sample to Seq The first part of the measurement process is generating sequencing data from an environmental sample. The part consists of five main steps:  Sample Collection  Sample Processing  16S rRNA PCR  Library Prep  Sequencing   4.1.1 Sample Collection  Collection of sample that is representative of population being studied.  Kitty Biome study as an example of sample heterogeneity.  Impact of sample storage.  Fecal storage study.  Old coral study?    4.1.2 Sample Processing  DNA extraction  Cell type specific extraction biases  DNA concentration requirements for library prep  Issues with low concentration samples - skin microbiome  Additional DNA purification to remove PCR inhibitors.  Old PCR inhibitor paper.  Any studies showing inhibitors inducing biases in metagenomic studies?  Potentially sample specific biases but do not know of any within sample biases     4.1.3 16S rRNA PCR  Target variable region  PCR primers (optional barcoded primers) PCR reagents  Polymerase - high fidelity  Reagent contaminats  PCR cycling conditions  number of cycles sequence copy abundance  chimeras    4.1.4 Library Preparation  Barcode/ indexing (optional)  Do not know of any barcode specific biases Normalization and Pooling  Methods  DNA quantification  Bead based assays  Associated Biases  Uneven sequencing depth - impact of coverage     4.1.5 Sequencing  Sequencing platform  Read length  Sequencing chemistry     4.2 Seq to OTU Convering raw sequence data to a OTU table with taxonomic annotations for use in downstream applications.  4.2.1 quality filter/ read trimming  Filtering low quality reads - short reads with high error rates.  Trimming reads - removing ends of reads based on base quality score as well as removal of adapter sequences that were added during sample processing. Merging paired ends Merging forward and reverse reads into single contigs chimera detection detection and removal of chimeric reads, artifact of the PCR amplification process.    4.2.2 OTU Assignment OTU assignment most commonly by clustering is used to reduce the number of spurious observations, sequences are grouped into OTUs.  Different approaches to clustering  distance based clustering - most commonly used  Distance based clustering is a NP hard problem (i.e. computationaly intense), therefore a number of heuristics are used to make the method computationally tractable for large datasets. Threshold are used to define the similarity of sequences within a cluster, 99% and 97% are commonly used.  Some biological and sequencing error basis for threshold values, (e.g. 99% species level, 97% genus level, 1% error rate). Taxonomic variability in 16S rRNA sequence diversity (Pai?) and sequencing error (Huse et al. 2010, Schirmer et al. (2015), Amore et al. (2016)) challenges the validity of these values.  Different distance based thresholds.  average linkage  single linkage  complete linkage   Other clustering/ OTU assignment methods include: distribution based  oligotyping  tree based clustering (Wu, Doroud, and Eisen 2013) De novo vs Reference based clustering Sequences can be clustered with a set of reference cluster centers or without (de novo).     4.2.3 Taxonomic Assignment Assign clusters a taxonomic name    4.3 Downstream analysis  4.3.1 Differential Abundance  Differences in taxa or OTU abundance between experimental factors, e.g. treatment vs. control.    4.3.2 Alpha Diversity  Measure of biological or ecological diversity within a sample.  Alpha diversity as a continuum related to the metric dependence on richness and evenness (Chao et al. 2014) Four primary categories richness, evenness, abundance weighted, and dissimilarity (Ricotta 2007) Richness  Hill numbers, rarefaction, and richness estimation (Chao et al. 2014)  Evenness  Criteria for evaluating evenness metrics (Tuomisto 2012) See Tuomisto (2012) Table 1 for summary of evenness metric including interpretation  Abundance weighted Dissimilarity Parametric vs. Non-parametric diversity metrics for 16S rRNA sequence analysis (Bunge, Willis, and Walsh 2014) Development of alpha diversity richness metric for microbial diversity.  Chao1 a traditional diversity metric estimates total diversity using the number of singletons, OTUs representing only a single sequence.  Issue with singleton based diversity metrics for microbial diversity is that for incresed sequencing depth the number of singletons with continue to increase due to sequencing errors, leading to artifically inflated diversity estimates.  Chiu and Chao (2015) developed a modified version of the Chao1 estimator that is more robust to singletons bue to sequencing errors. This method estimates diversity based on the number of doubletons to tentons.  Incorporates Hill numbers as well.    4.3.3 Beta Diversity     4.4 16S rRNA Analysis Method Assessment Method steps 1. Sample to Seq 1. Seq to OTU 2. Downstream analysis 3. Taxonomic Classification 4. Differential Abundance 5. Ecological Diversity   4.5 Sample to fastq  Benchmarking  using in vitro mock communities (e.g. cells, DNA, or PCR products)  technical replicates?  well characterized benchmarking samples - similar to Genomic RMs (deep sequenced multiple methods)  Mixture/ titrations  Standard additions  Dilution to extinction?     4.6 Seq to OTU  4.6.1 Pre-processing Schloss, P. D. (2010). The Effects of Alignment Quality, Distance Calculation Method, Sequence Filtering, and Region on the Analysis of 16S rRNA Gene-Based Studies. PLoS Comput Biol, 6(7), e1000844. http://doi.org/10.1371/journal.pcbi.1000844 Albanese, D., Fontana, P., De Filippo, C., Cavalieri, D., &amp; Donati, C. (2015). MICCA: a complete and accurate software for taxonomic profiling of metagenomic data. Scientific Reports, 5, 9743. http://doi.org/10.1038/srep09743 * Evaluates preprocessing % read passing filtering, chimeras, and redundant OTUs, relative abundance Gaspar, J. M., &amp; Thomas, W. K. (2013). Assessing the consequences of denoising marker-based metagenomic data. PloS One, 8(3), e60458. http://doi.org/10.1371/journal.pone.0060458 * Evaluation of denoising and chimera detection methods for 454 data   4.6.2 Chimera Detection Mysara, M., Saeys, Y., Leys, N., Raes, J., &amp; Monsieurs, P. (2015). CATCh, an Ensemble Classifier for Chimera Detection in 16S rRNA Sequencing Studies. Applied and Environmental Microbiology, 81(December), 1573–1584. http://doi.org/10.1128/AEM.02896-14 Beblur - https://github.com/ekopylova/deblur   4.6.3 OTU Assignment  De novo clustering has been shown to be more accurate (S. L. Westcott and Schloss 2015)  Types of datasets used to assess benchmarking methods: simulated data, mock communities, environmental datasests Approaches to evaluating clustering methods  Cluster stability - similarity in clustering results when varying input data  Cluster reproducibility - similarity in clustering results between methods  Number of artifact OTUs - number of observed OTUs relative to expected  Evaluating clustering methods using simulated and mock community clustering data provide a level of truth (expectation of the correct result).     4.6.3.1 Summary of Cluster Method Assessment Papers  Kopylova et al. (2014) used a combination of simulated, mock communities, and data from environmental samples to compare clustering methods.  They evaluated the performance of the clustering methods using F-measure and phylogenetic distance.  For environment datasets the clustering methods were evaluated based on the disimilarity of samples for a dataset based on the sum of the squared deviation for UniFrac PCoA (Procrustes M^2) and similarity to UCLUST (Pearson’s correlation). NOTE Still note sure what they are evaluating for Procrustes.  The authors excluded singletons from there analysis, while this does help to eliminate spurious OTUs, the method may also exclude rare taxa.   Schloss (2016) argues that using simulated and mock communities is not representative of real environmental samples. The authors additionally, argue the approach used by Kopylova et al. (2014) to evaluating clustering methods confounds the impact of read filtering with clustering methods.  The author’s evaluated environmental datasets using Mathew’s correlation coefficient, for a truth table (TP, TN, FP, FN) based on distance between sequences in the same and different clusters.  This method for evaluating clusters was first used in (P. D. Schloss and Westcott 2011)   Average neighbor performed the best based the MCC based assessment. However, this approach to evaluation is biased towards this method as the results are strictly assessed based on the sequence distance criteria. Other clustering methods that attempt to address more nuianced issues with clustering, sequencing error and variability in sequence diversity allow for differences in distances within and between clusters.  This approach to cluster evaluation is particularly suited for algorithms based on analytical proofs. For example the DNACluster clustering algorithm employs a heuristic based on a proof that for any given cluster the distance between two sequences in the cluster is less than the defined threshold value and no sequence is closer to another cluster center than the center of the cluster it is assigned to (Ghodsi, Liu, and Pop 2011).  Ghodsi, Liu, and Pop (2011) used a similar approach to validate their clustering method, though only focusing on the within cluster pairwise distance.   Cluster Robustness is another attribute for evaluating clustering methods (He et al. 2015).  Cluster robustness is the reproduciblity of the OTU table generated by a clustering algorith.  Cluster robustness was evaluated based on:  Unstable sequences - sequences represented by different centroids Unstable OTUs - OTUs where membership was impacted by the number of sequences in a dataset. using Mathew’s correlation coefficient  Truth Table definitions  TP if two sequences clustered together for the full and subsampled dataset  TN if two sequences clustered separately for the full and subsampled  FN two sequences clustered together in full but not subsampled.  FP two sequences clustered separately in full but together in subsampled dataset.  The MCC method uses the full dataset at the true value to benchmark the subsampled dataset against.  NOTE Is this a valid assumption for clustering?   Impact of cluster stablity on Rarefaction curves, PCoA with Bray-Curtis and UniFrac.  Tested for significance using ADNOIS, a non-parametric test measuring effect size, the amount of the observed variance explained by metadata variables. NEED REF - See vegan manual for potenial references.  The resulting OTU table is dependent the number of sequences clustered.  For some methods the OTU table is dependent on the order in which the sequences are provided (Ghodsi, Liu, and Pop 2011).  Closed reference clustering was the only method the produced stable clusters.  This method discards sequences that do not cluster with reference sequences.  Open-reference cluster, which performs de novo clustering of unmatched sequences provides a more robust alternative clustering method to de novo clustering while allowing for the diversity of novel OTUs.   Cluster method robustness can also be calculated using cross validation(Chen et al. 2013).  Chen et al. (2013) subsampled the data to 90% five times and compared the clusters between replicates.  Methods were evaluated using precision, recall, and NID  precision - measure of cluster homogeneity  recall - measure of cluster completeness  NID - assess cluster globally???   See assessment of cluster quality for metric definitions   Cluster robustness can also be evaluated based on threshold and 16S region (Schmidt, Matias Rodrigues, and Mering 2015).  The authors focus their assessment more on the comparability of results among different methods and note the importances of reproducibility.  Quote from paper “how robust are biological findings to the choice of clustering method? We found that OTU demarcation may indeed be replicable: different methods provided (almost) identical partitions when twice clustering the exact same sets of sequences, but in randomized order (Fig. 4, diagonals). However, trends in reproducibility were less clear.”   Cluster method assessment based on OTU inflation.  Clustering methods have also been evaluated based on the number of predicted OTUs relative to the number of expected OTUs (Huse et al. 2010, Kopylova et al. (2014)).  Here either simulated or data from a mock community, where the number of OTUs is based on the number of strains or genomes used to generate the assessment dataset.  The limitation to this approach is that it does not account for contaminants when in vitro mock communities are used, and neither in silico or in vitro datasets represent the true complexity and diversity of environmental samples.       4.7 Downstream analysis  4.7.1 taxonomic classification   4.7.2 differential abundance   4.7.3 ecological diversity    "],
["benchmarking-dataset.html", "5 Benchmarking Dataset", " 5 Benchmarking Dataset  Mixture Study   "],
["ecological-diversity-robustness.html", "6 Ecological Diversity Robustness 6.1 Overview", " 6 Ecological Diversity Robustness  6.1 Overview  Background Evaluating alpha and beta diversity metrics  Factors  preprocessing methods  data characteristics  sequencing error rates  primer region  read length    OTU table  depth/ coverage - sparsity  when does the diversity metric break?     "],
["gold-standard-pipeline.html", "7 Gold Standard Pipeline", " 7 Gold Standard Pipeline  Trim primers - dereplicate  Chimera check? - multiple methods orthogonal methods?, annotate don’t filter  BLAST 100% - all perfect matches  Unmatched  Multiple sequence alignment  Filter sequences with low alignment scores   phylogenetic placement   Generate OTUs for different branch distances  Keep unique singletons - based on distance from non-singleton clusters, vary similarity based on cluster size, likelihood model for whether sequencing error or biological mutation.  Rare orgs highly similar to large OTU less likely ecologically different?   Alternative taxa classification  Multiple methods  pairwise alignment  kmer  phylogenetic placement  other…   Multiple reference databases  evaluate individual sequences based on confidence - presence in multiple databases, infernal and rnammer alignment score?  supporting sequence data - whole genome, sanger, 16S PCR from isolate  likelihood of being a chimera   Compare other methods to results generated using gold standard pipeline  performance metrics     Copy number correction  based on phylogenetic placement    "],
["assessment-framework.html", "8 Assessment Framework", " 8 Assessment Framework  Outline measurement process - fishbone diagram Differentiate between biological variability and measurement error  How are these two sources of variability currently addressed in pipelines Are there any known sources of biological variability not currently being addressed  What is known about each source of variability?  uncertainty budget?  dark uncertianty - unaccounted for variability?  Robustness testing  determine largest source of bias/ uncertianty in measurement process  Schmidt 2014 Repeatability and Reproducibility Evaluation - no gold standard method, or known truth  Quantifying similarity in results  Schmidt et al 2014 Env Micro Schloss et al 2016  Method with similar assumptions Methods with different assumption   Charaterize individual measurement process steps based on robustness testing/ sensitivity analysis  benchmarking datasets  environmental  in lab prepared material - combination of known samples  in lab prepared material - combination of unknown samples  in silico - various levels of simulation based on reference and environmental data availablility   for each type of dataset summarize benefits and limitations to each  Evaluation Framework  Requirements  Benchmarking dataset  Metric for evaluating performance  Assumption Based vs. Emerical Assessment  Schloss et al. paper using within and between cluster distances to benchmark clustering methods   Thinking about what is being measured - biological question    Comparison of methods  Benchmarking  Gold standard method - known correct answer  For new method if different how to determine if new method is better?   Reference dataset - known truth   Evaluation Methods  Benchmarking  Comparing results to known truth  limitation with biological applications no real ground truth.  For physical and chemical metrology a ground truth is determined through uncertainty analysis with traceability to the SI. Often requiring the use of two orthogonal methods and uncertainty budgets. Currently no methods for traceing sequence data to an SI.    Benchmarking datasets are generated from materials or in silico with a level of confidence regarding expected results.  Discussion of top down vs. bottom up uncertainty analysis for use in establishing truth with benchmarking datasets.   Results from analysis of benchmarking datasets evaluated using performance metrics, e.g. truth tables for qualitative data.   Benchmarking- optimization  evaluating method performance when subjected to changes in either quality of data (increased noise) or use of different methods or parameters (optimization) similar to sensitivity analysis but comparing results to a dataset with known truth.      "],
["references.html", "9 References", " 9 References    Amore, Rosalinda D, Umer Zeeshan Ijaz, Melanie Schirmer, John G Kenny, Richard Gregory, Alistair C Darby, Christopher Quince, et al. 2016. “A comprehensive benchmarking study of protocols and sequencing platforms for 16S rRNA community profiling.” BMC Genomics 17. BMC Genomics: 1–40. doi:10.1186/s12864-015-2194-9.   Bunge, John, Amy Willis, and Fiona Walsh. 2014. “Estimating the Number of Species in Microbial Diversity Studies.” Annual Review of Statistics and Its Application 1 (1): 427–45. doi:10.1146/annurev-statistics-022513-115654.   Chao, Anne, Nicholas J. Gotelli, T. C. Hsieh, Elizabeth L. Sander, K. H. Ma, Robert K. Colwell, and Aaron M. Ellison. 2014. “Rarefaction and extrapolation with Hill numbers: A framework for sampling and estimation in species diversity studies.” Ecological Monographs 84 (1): 45–67. doi:10.1890/13-0133.1.   Chen, Wei, Clarence K. Zhang, Yongmei Cheng, Shaowu Zhang, and Hongyu Zhao. 2013. “A Comparison of Methods for Clustering 16S rRNA Sequences into OTUs.” PLoS ONE 8 (8). doi:10.1371/journal.pone.0070837.   Chiu, Chun-Huo, and Anne Chao. 2015. “Estimating and comparing microbial diversity in the presence of sequencing errors.” PeerJ, 1–13. doi:10.7287/peerj.preprints.11.   Ghodsi, Mohammadreza, Bo Liu, and Mihai Pop. 2011. “DNACLUST: accurate and efficient clustering of phylogenetic marker genes.” BMC Bioinformatics 12 (1). BioMed Central Ltd: 271. doi:10.1186/1471-2105-12-271.   He, Yan, J Gregory Caporaso, Xiao-Tao Jiang, Hua-Fang Sheng, Susan M Huse, Jai Ram Rideout, Robert C Edgar, et al. 2015. “Stability of operational taxonomic units: an important but neglected property for analyzing microbial diversity.” Microbiome 3 (1). ??? 20. doi:10.1186/s40168-015-0081-x.   Huse, Susan M, David Mark Welch, Hilary G Morrison, and Mitchell L Sogin. 2010. “Ironing out the wrinkles in the rare biosphere through improved OTU clustering.” Environmental Microbiology 12 (7): 1889–98. doi:10.1111/j.1462-2920.2010.02193.x.   Kopylova, Evguenia, Jose A Navas-molina, Céline Mercier, and Zech Xu. 2014. “Open-Source Sequence Clustering Methods Improve the State Of the Art.” MSystems 1 (1): 1–16. doi:10.1128/mSystems.00003-15.Editor.   Ricotta, Carlo. 2007. “A semantic taxonomy for diversity measures.” Acta Biotheoretica 55 (1): 23–33. doi:10.1007/s10441-007-9008-7.   Schirmer, Melanie, Umer Z. Ijaz, Rosalinda D’Amore, Neil Hall, William T. Sloan, and Christopher Quince. 2015. “Insight into biases and sequencing errors for amplicon sequencing with the Illumina MiSeq platform.” Nucleic Acids Research 43 (6). doi:10.1093/nar/gku1341.   Schloss, Patrick D. 2016. “Application of database-independent approach to assess the quality of OTU picking methods.”   Schloss, Patrick D., and Sarah L. Westcott. 2011. “Assessing and Improving Methods Used in Operational Taxonomic Unit-Based Approaches for 16S rRNA Gene Sequence Analysis.” Applied and Environmental Microbiology 77 (10): 3219–26. doi:10.1128/AEM.02810-10.   Schmidt, Thomas S B, Jo??o F. Matias Rodrigues, and Christian von Mering. 2015. “Limits to robustness and reproducibility in the demarcation of operational taxonomic units.” Environmental Microbiology 17 (5): 1689–1706. doi:10.1111/1462-2920.12610.   Tuomisto, Hanna. 2012. “An updated consumer’s guide to evenness and related indices.” Oikos 121 (8): 1203–18. doi:10.1111/j.1600-0706.2011.19897.x.   Westcott, Sarah L, and Patrick D Schloss. 2015. “De novo clustering methods out-perform reference-based methods for assigning 16S rRNA gene sequences to operational taxonomic units.” PeerJ 3: e1487. doi:10.7287/peerj.preprints.1466v1.   Wu, Dongying, Ladan Doroud, and Jonathan a. Eisen. 2013. “TreeOTU Operational Taxonomic Unit Classification Based on Phylogenetic Trees.” Arxiv.Org, 23. http://arxiv.org/abs/1308.6333.   "]
]
