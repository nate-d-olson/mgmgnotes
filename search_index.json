[
["index.html", "Measurement Science of Metagenomics 1 Metrology of Metagenomics 1.1 Microbiome 1.2 Measuring Microbiomes 1.3 Metrology 1.4 Key references", " Measurement Science of Metagenomics   1 Metrology of Metagenomics  1.1 Microbiome   1.2 Measuring Microbiomes  whole metagenomic sequencing  marker-gene metagenomics  16S vs. WMS  Clooney et al. (2016) compared WMS and 16S rRNA sequence data for environmental samples sequenced using two platforms MiSeq and PGM.  WMS clustering seperately form the 16S results (Clooney et al. 2016, fig. 1).     1.3 Metrology  No not meteorlogy  Biological variability and measurement error  Measurement Error  Systematic - results in systematic bias  Random - increases uncertainty in results but provides an accurate measure of the true mean given sufficient sample size   Biological Noise  Sources of variability due to biological complexity of the system e.g. difference in results based on 16S rRNA gene region    replicate, repeat, and reproduce  SI redefining the kilogram example???  Uncertainty budgets  top-down vs. bottom-up  analytical chemsitry text   Orthogonal methods  Genome in a bottle example  Examples of orthogonal methods for 16S metagenomics:  Albertsen et al. (2015) 16S, metagenomics, metatranscriptomics, and FISH      1.4 Key references  1.4.1 Measurement Process Goodrich et al. (2014)   1.4.2 Sample Processing and Sequencing Brooks et al. (2015) D’Amore et al. (2016)  most robust method       - Benchmarking           + Observed results vs. Truth               * Dataset with known true value - can be obtained using                   - gold standard method                   - mock communities: in vitro or in silico                        + limitations and assumptions                   - Mixtures                     + Provides a degree of known truth               * Performance metrics                   - Quantitative                       + Variability - standard deviation/ variance                       + Bias - mean squared error??? (true value)           + Observed results vs. assumptions               * No general dataset requirements               * Performance metrics                   - evaluate results based on method assumption, e.g. Schloss clustering distance       - Robustness Testing           + Used to evaluate impact of different methods on results           + Comparison of orthogonal methods               * For fundamentally different methods (different assumptions and heuristics)                   - Can evaluate precision (variability), not necessarily accuracy but less susceptible to bias than is methods are based on the same assumptions or heuristics           + Used to evaluate how different steps in the process affect the results    ## Microbiome Mixture Benchmarking Dataset * Mixture Study    ## Robustness Testing Ecological Diversity  * Evaluating alpha and beta diversity metrics   * Factors       - preprocessing methods       - data characteristics           + sequencing error rates           + primer region           + read length   * OTU table       - depth/ coverage - sparsity       - when does the diversity metric break?    ## Guideance on developing an assessment framework * Outline measurement process - fishbone diagram   * Differentiate between biological variability and measurement error       - How are these two sources of variability currently addressed in pipelines       - Are there any known sources of biological variability not currently being addressed   * What is known about each source of variability?       - uncertainty budget?       - dark uncertianty - unaccounted for variability?   * Robustness testing       - determine largest source of bias/ uncertianty in measurement process   * Charaterize individual measurement process steps based on robustness testing/ sensitivity analysis       - benchmarking datasets           + environmental           + in lab prepared material - combination of known samples           + in lab prepared material - combination of unknown samples           + in silico - various levels of simulation based on reference and environmental data availablility       - for each type of dataset summarize benefits and limitations to each    ## Proposed Gold Standard Analysis Method * Trim primers - dereplicate   * Chimera check? - multiple methods orthogonal methods?, annotate don't filter   * BLAST 100% - all perfect matches   * Unmatched       - Multiple sequence alignment           + Filter sequences with low alignment scores       - phylogenetic placement   * Generate OTUs for different branch distances   * Keep unique singletons - based on distance from non-singleton clusters, vary similarity based on cluster size, likelihood model for whether sequencing error or biological mutation.       - Rare orgs highly similar to large OTU less likely ecologically different?   * Alternative taxa classification       - Multiple methods           + pairwise alignment           + kmer           + phylogenetic placement           + other...       - Multiple reference databases           + evaluate individual sequences based on confidence - presence in multiple databases, infernal and rnammer alignment score?               * supporting sequence data - whole genome, sanger, 16S PCR from isolate               * likelihood of being a chimera           + Compare other methods to results generated using gold standard pipeline               * performance metrics   * Copy number correction       - based on phylogenetic placement -->    "],
["s-rrna-gene.html", "2 16S rRNA Gene 2.1 General background about 16S rRNA Gene 2.2 16S rRNA phylogenetics 2.3 16S rRNA Databases", " 2 16S rRNA Gene Not sure where this fits in…  2.1 General background about 16S rRNA Gene  Biological function  Variable and conserved regions  Multicopy  Horizontal gene transfer  Gene diversity  secondary structure  Use of different variable regions     2.2 16S rRNA phylogenetics  Gene evolution  Phylogeny    2.3 16S rRNA Databases  RDP  Greengenes  NCBI  SILVA    "],
["process-characterization.html", "3 Process Characterization", " 3 Process Characterization  Tools  Validation  Fit for purpose - comparison to gold standard method or reference material  no gold standard methods - only community accepted defactor standards    Benchmarking  Comparison to known truth  Evaluating results based on expectation   Robustness testing  Impact of procedure or step on results    Characterization of Data Analysis Processing  Wet lab vs. dry lab  Validation  Currently not applicable … -&gt; most robust method   Benchmarking  Observed results vs. Truth  Dataset with known true value - can be obtained using  gold standard method  mock communities: in vitro or in silico  limitations and assumptions   Mixtures  Provides a degree of known truth    Performance metrics  Quantitative  Variability - standard deviation/ variance  Bias - mean squared error??? (true value)     Observed results vs. assumptions  No general dataset requirements  Performance metrics  evaluate results based on method assumption, e.g. Schloss clustering distance     Robustness Testing  Used to evaluate impact of different methods on results  Comparison of orthogonal methods  For fundamentally different methods (different assumptions and heuristics)  Can evaluate precision (variability), not necessarily accuracy but less susceptible to bias than is methods are based on the same assumptions or heuristics    Used to evaluate how different steps in the process affect the results     "],
["measurement-process-1.html", "4 Measurement Process 4.1 Sample to Seq 4.2 Seq to OTU 4.3 Downstream analysis 4.4 Step-by-Step protocols", " 4 Measurement Process Overview of the 16S rRNA marker gene metagenomics measurement process including the sources of error and bias introduced at different step in the process along with different mitigation strategies that have been developed to address each of the issues. See Goodrich et al. (2014) for a review of 16S rRNA marker gene metagenomics measurement process and general recomendations for conducting a microbiome study. J. Zhou et al. (2011) Used biological and technical replicates with a Shewanella oneidenis MR-1 to assess the reproducibility and quanitative accuracy of 16S sequencing, using 454 pyroseuqencing and targeting the V45 region. Results demonstrated a lack of reporducibility and quantiative accuracy when comparing the \\(\\beta\\) diversity and relative abundance between replicates. Diagram of measurement process 4.1. ## Package &#39;qcc&#39;, version 2.6 ## Type &#39;citation(&quot;qcc&quot;)&#39; for citing this R package in publications.    Figure 4.1: Fishbone diagram of the 16S rRNA metagenomic sequencing measurement process.   Review of what is currently known about the sources of error and bias in the 16S metagenomics measurement process. Cause-effect diagrams are commonly used in process quality control to show the relationship between steps in the measurement process and sources of error or bias 4.2. Potentially may want to revise to focus on sources of error, e.g. cell type bias, preferential amplification, sequence errors, chimeras .    Figure 4.2: Causes (branch labels) of bias and variability indicated for different sources of error (branch text).    4.1 Sample to Seq The first part of the measurement process is generating sequencing data from an environmental sample. The part consists of five main steps:  Sample Collection  Sample Processing  16S rRNA PCR  Library Prep  Sequencing   4.1.1 Sample Collection  Collection of sample that is representative of population being studied  Kitty Biome study as an example of sample heterogeneity   Impact of sample storage  Fecal storage study  Old coral study?     4.1.2 Sample Processing  4.1.2.1 DNA extraction Considerations:  Cell type specific extraction biases  DNA concentration requirements for library prep  Issues with low concentration samples - skin microbiome   Extra cellular DNA PCR inhibitiors  Old PCR inhibitor paper.  Any studies showing inhibitors inducing biases in metagenomic studies?  Potentially sample specific biases but do not know of any within sample biases    Albertsen et al. (2015) evalauted the impact of different DNA extraction methods for activated slude communities. Bead beating both time and intensity impacted the yield and size distribution of the extracted DNA as well as the observed abundance of different bacteria at both the phylum level and class for Proteobacteria.    4.1.3 16S rRNA PCR  4.1.3.1 Template concentration  This is related to low concentration samples or samples with poor DNA extraction efficiencies.  Kennedy et al. (2014) showed that the profile of replicate samples for different template concentration was more variable then when the imput concentration is the same. Pooling replicate PCR reactions decreased the within sample variablility. D’Amore et al. (2016) increasing the template concentration from 1 ng to 10 ng for MiSeq lead to higher percentages of chimeric reads 0.08 compared to 0.2% (p = 0.20).   4.1.3.2 Target variable region Clooney et al. (2016) Compared sequencing data between platforms and variable regions. Based on the relative abundance of genera present in 20% of the datasets (including whole metagenome sequencing (WMS)) the V12 and V45 primer regions, PGM sequencing data was less impacted than MiSeq to different variable regions. Kozich et al. (2013) compared the results for three different variable regions sequenced using the MiSeq platform. The three regions, V34, V4, and V45 represented different length (250 bp, 430 bp, and 375 bp) though overlapping target regions. The observed error rate varied between run as well as variable regions resulting in differences in predicted OTUs. V45 had the highest predicted OTUs for all samples and replicate sequencing runs, followed by V34 and V4. The number of observed OTUs was higher for all datasets than expected for the mock community datasets.   4.1.3.3 PCR primers (optional barcoded primers) Albertsen et al. (2015) performed both an in silico and in vitro comparison of primer regions as well as comparing the relative abundance of different Phyla to metagenomic and metatranscriptomic datasets. The author’s concluded by recomending V13 for activated sludge samples based on what is known about the community composition of the system.   4.1.3.4 PCR reagents  Polymerase - high fidelity  Reagent contaminats    4.1.3.5 PCR cycling conditions   4.1.3.6 number of cycles Using a mock community of 68 organisms D’Amore et al. (2016) showed that increasing the number of PCR cycles resulted in increased sequencing error rates (0.58% vs. 0.64% p = 0.11) and chimeras (15 cycles 0.00% and 25 cycles 0.66%, p = 0.0245). Albertsen et al. (2015) evaluated the amount of template and cycle number but only observed “small changes”. Decreasing the annealing temperatures of 58C and 52C resulted in increased abundance of prevously low abundance OTU with no impact on high abundance OTUs. Yu et al. (2015) compared standard and nest PCR (barcodes used in the second round), showed that the number of cycles and use of separate PCR and indexing reactions influenced the number of observed OTUs and \\(\\beta\\) diversity.   4.1.3.7 sequence copy abundance   4.1.3.8 chimeras    4.1.4 Library Preparation Kennedy et al. (2014) found that variability due to PCR (particularly template concentration) resulted in less variability for within sample results than sample preparation (library preparation) and inter-lane variability. Barcoded 16S PCR primers were used in the study therefore the impact of barcodes and variability of 16S PCR was confounded. Schirmer et al. (2015) sequenced a mock community of 59 different strains to evaluate the sequencing error rate for different sample processing methods. Though only fully characterizing a subset of a the results.  4.1.4.1 Barcode/ indexing (optional)  Do not know of any barcode specific biases    4.1.4.2 Normalization and Pooling  Methods  DNA quantification  Bead based assays   Associated Biases  Uneven sequencing depth - impact of coverage      4.1.5 Sequencing  Sequencing platform  Clooney et al. (2016) compared 16S rRNA sequence data for environmental samples sequenced using two platforms MiSeq and PGM.  * Comparing results between stool samples from six individuals based on the relative abundance of genera present in 20% of the datasets (including whole metagenome sequencing (WMS)) the datasets clustered together based on sequencing methods (platform and primers) rather than individual, with WMS clustering seperately form the 16S results (Clooney et al. 2016, fig. 1).  Read length  Sequencing chemistry     4.2 Seq to OTU Convering raw sequence data to a OTU table with taxonomic annotations for use in downstream applications.  4.2.1 quality filter/ read trimming  Filtering low quality reads - short reads with high error rates.  Trimming reads - removing ends of reads based on base quality score as well as removal of adapter sequences that were added during sample processing. Merging paired ends Merging forward and reverse reads into single contigs chimera detection detection and removal of chimeric reads, artifact of the PCR amplification process.    4.2.2 OTU Assignment OTU assignment most commonly by clustering is used to reduce the number of spurious observations, sequences are grouped into OTUs.  Different approaches to clustering  distance based clustering - most commonly used  Distance based clustering is a NP hard problem (i.e. computationaly intense), therefore a number of heuristics are used to make the method computationally tractable for large datasets. Threshold are used to define the similarity of sequences within a cluster, 99% and 97% are commonly used.  Some biological and sequencing error basis for threshold values, (e.g. 99% species level, 97% genus level, 1% error rate). Taxonomic variability in 16S rRNA sequence diversity (Pai?) and sequencing error (Huse et al. 2010, Schirmer et al. (2015), D’Amore et al. (2016)) challenges the validity of these values.  Different distance based thresholds.  average linkage  single linkage  complete linkage    Other clustering/ OTU assignment methods include:  distribution based  oligotyping  tree based clustering (D. Wu, Doroud, and Eisen 2013)  De novo vs Reference based clustering  Sequences can be clustered with a set of reference cluster centers or without (de novo).      4.2.3 Taxonomic Assignment Assign clusters a taxonomic name * (“Comparison of two next-generation sequencing technologies for resolving highly complex microbiota composition using tandem variable 16S rRNA gene regions.” 2010) in silico analysis of the taxonomic resolution for different variable regions and read lengths (454 and MISeq 150, 100, 75, and 50 bp paired end reads) and primers.    4.3 Downstream analysis  4.3.1 Differential Abundance  Differences in taxa or OTU abundance between experimental factors, e.g. treatment vs. control. Jonsson et al. (2016) Simulation based approach to evaluating differential abundance methods. NEED TO REVIEW    4.3.2 Alpha Diversity  Measure of biological or ecological diversity within a sample.  Alpha diversity as a continuum related to the metric dependence on richness and evenness (Chao et al. 2014) Four primary categories richness, evenness, abundance weighted, and dissimilarity (Ricotta 2007)  Richness  Hill numbers, rarefaction, and richness estimation (Chao et al. 2014)  Evenness  Criteria for evaluating evenness metrics (Tuomisto 2012) See Tuomisto (2012) Table 1 for summary of evenness metric including interpretation  Abundance weighted Dissimilarity  Parametric vs. Non-parametric diversity metrics for 16S rRNA sequence analysis (Bunge, Willis, and Walsh 2014) Development of alpha diversity richness metric for microbial diversity.  Chao1 a traditional diversity metric estimates total diversity using the number of singletons, OTUs representing only a single sequence.  Issue with singleton based diversity metrics for microbial diversity is that for incresed sequencing depth the number of singletons with continue to increase due to sequencing errors, leading to artifically inflated diversity estimates.  Chiu and Chao (2015) developed a modified version of the Chao1 estimator that is more robust to singletons bue to sequencing errors. This method estimates diversity based on the number of doubletons to tentons.  Incorporates Hill numbers as well.     4.3.3 Beta Diversity    4.4 Step-by-Step protocols  Albertsen et al. (2015) Activated Sludge http://www.midasfieldguide.org/    "],
["s-rrna-analysis-method-assessment.html", "5 16S rRNA Analysis Method Assessment 5.1 Sample to fastq 5.2 Seq to OTU 5.3 Downstream analysis", " 5 16S rRNA Analysis Method Assessment Method steps 1. Sample to Seq 1. Seq to OTU 2. Downstream analysis 3. Taxonomic Classification  4. Differential Abundance  5. Ecological Diversity  5.1 Sample to fastq  Benchmarking  using in vitro mock communities (e.g. cells, DNA, or PCR products)  technical replicates?  well characterized benchmarking samples - similar to Genomic RMs (deep sequenced multiple methods)  Mixture/ titrations  Standard additions  Dilution to extinction?   Experimental design  Brooks et al. (2015) generated 80 mock communities using different combinations of 7 bacterial strains generating mixtures of cells, DNA, and PCR products to characterize the contributions of different steps in the measurment process. Evalualted the results using the mixture effect model.    5.2 Seq to OTU  5.2.1 Pre-processing  Single org samples  Data sets from sequencing individual organisms using a standard 16S metagenomic pipeline had been used to evaluate sequencing errors and optimize filtering parameters.  Kunin et al. (2010) evaluated the sequencing error rates and use of different filtering thresholds for 454 pyrosequencing of an E. coli isolated. Schloss, P. D. (2010). The Effects of Alignment Quality, Distance Calculation Method, Sequence Filtering, and Region on the Analysis of 16S rRNA Gene-Based Studies. PLoS Comput Biol, 6(7), e1000844. http://doi.org/10.1371/journal.pcbi.1000844  Multiple Org  Schirmer et al. (2015) using a mock community of 59 strains (49 bacteria and 10 archaea) evaluated differnt sequence processing methods and parameters for read filtering and quality trimming, merging paired end reads, and error correction. Methods were evaluated based on error rate reduction.   Albanese, D., Fontana, P., De Filippo, C., Cavalieri, D., &amp; Donati, C. (2015). MICCA: a complete and accurate software for taxonomic profiling of metagenomic data. Scientific Reports, 5, 9743. http://doi.org/10.1038/srep09743 * Evaluates preprocessing % read passing filtering, chimeras, and redundant OTUs, relative abundance Gaspar, J. M., &amp; Thomas, W. K. (2013). Assessing the consequences of denoising marker-based metagenomic data. PloS One, 8(3), e60458. http://doi.org/10.1371/journal.pone.0060458 * Evaluation of denoising and chimera detection methods for 454 data   5.2.2 Chimera Detection Mysara, M., Saeys, Y., Leys, N., Raes, J., &amp; Monsieurs, P. (2015). CATCh, an Ensemble Classifier for Chimera Detection in 16S rRNA Sequencing Studies. Applied and Environmental Microbiology, 81(December), 1573–1584. http://doi.org/10.1128/AEM.02896-14 Beblur - https://github.com/ekopylova/deblur   5.2.3 OTU Assignment  De novo clustering has been shown to be more accurate (S. L. Westcott and Schloss 2015)  Types of datasets used to assess benchmarking methods: simulated data, mock communities, environmental datasests Approaches to evaluating clustering methods  Cluster stability - similarity in clustering results when varying input data  Cluster reproducibility - similarity in clustering results between methods  Number of artifact OTUs - number of observed OTUs relative to expected  Evaluating clustering methods using simulated and mock community clustering data provide a level of truth (expectation of the correct result).     5.2.3.1 Summary of Cluster Method Assessment Papers  Kopylova et al. (2014) used a combination of simulated, mock communities, and data from environmental samples to compare clustering methods.  They evaluated the performance of the clustering methods using F-measure and phylogenetic distance.  For environment datasets the clustering methods were evaluated based on the disimilarity of samples for a dataset based on the sum of the squared deviation for UniFrac PCoA (Procrustes M2) and similarity to UCLUST (Pearson’s correlation). NOTE Still note sure what they are evaluating for Procrustes.  The authors excluded singletons from there analysis, while this does help to eliminate spurious OTUs, the method may also exclude rare taxa.   Schloss (2016) argues that using simulated and mock communities is not representative of real environmental samples. The authors additionally, argue the approach used by Kopylova et al. (2014) to evaluating clustering methods confounds the impact of read filtering with clustering methods.  The author’s evaluated environmental datasets using Mathew’s correlation coefficient, for a truth table (TP, TN, FP, FN) based on distance between sequences in the same and different clusters.  This method for evaluating clusters was first used in (Schloss and Westcott 2011)   Average neighbor performed the best based the MCC based assessment. However, this approach to evaluation is biased towards this method as the results are strictly assessed based on the sequence distance criteria. Other clustering methods that attempt to address more nuianced issues with clustering, sequencing error and variability in sequence diversity allow for differences in distances within and between clusters.  This approach to cluster evaluation is particularly suited for algorithms based on analytical proofs. For example the DNACluster clustering algorithm employs a heuristic based on a proof that for any given cluster the distance between two sequences in the cluster is less than the defined threshold value and no sequence is closer to another cluster center than the center of the cluster it is assigned to (Ghodsi, Liu, and Pop 2011).  Ghodsi, Liu, and Pop (2011) used a similar approach to validate their clustering method, though only focusing on the within cluster pairwise distance.   Cluster Robustness is another attribute for evaluating clustering methods (Y. He et al. 2015).  Cluster robustness is the reproduciblity of the OTU table generated by a clustering algorith.  Cluster robustness was evaluated based on:  Unstable sequences - sequences represented by different centroids Unstable OTUs - OTUs where membership was impacted by the number of sequences in a dataset. using Mathew’s correlation coefficient  Truth Table definitions  TP if two sequences clustered together for the full and subsampled dataset  TN if two sequences clustered separately for the full and subsampled  FN two sequences clustered together in full but not subsampled.  FP two sequences clustered separately in full but together in subsampled dataset.  The MCC method uses the full dataset at the true value to benchmark the subsampled dataset against.  NOTE Is this a valid assumption for clustering?   Impact of cluster stablity on Rarefaction curves, PCoA with Bray-Curtis and UniFrac.  Tested for significance using ADNOIS, a non-parametric test measuring effect size, the amount of the observed variance explained by metadata variables. NEED REF - See vegan manual for potenial references.  The resulting OTU table is dependent the number of sequences clustered.  For some methods the OTU table is dependent on the order in which the sequences are provided (Ghodsi, Liu, and Pop 2011).  Closed reference clustering was the only method the produced stable clusters.  This method discards sequences that do not cluster with reference sequences.  Open-reference cluster, which performs de novo clustering of unmatched sequences provides a more robust alternative clustering method to de novo clustering while allowing for the diversity of novel OTUs.   Cluster method robustness can also be calculated using cross validation(Chen et al. 2013).  Chen et al. (2013) subsampled the data to 90% five times and compared the clusters between replicates.  Methods were evaluated using precision, recall, and NID  precision - measure of cluster homogeneity  recall - measure of cluster completeness  NID - assess cluster globally???   See assessment of cluster quality for metric definitions   Cluster robustness can also be evaluated based on threshold and 16S region (Schmidt, Matias Rodrigues, and Mering 2015).  The authors focus their assessment more on the comparability of results among different methods and note the importances of reproducibility.  Quote from paper “how robust are biological findings to the choice of clustering method? We found that OTU demarcation may indeed be replicable: different methods provided (almost) identical partitions when twice clustering the exact same sets of sequences, but in randomized order (Fig. 4, diagonals). However, trends in reproducibility were less clear.”   Cluster method assessment based on OTU inflation.  Clustering methods have also been evaluated based on the number of predicted OTUs relative to the number of expected OTUs (Huse et al. 2010, Kopylova et al. (2014)).  Here either simulated or data from a mock community, where the number of OTUs is based on the number of strains or genomes used to generate the assessment dataset.  The limitation to this approach is that it does not account for contaminants when in vitro mock communities are used, and neither in silico or in vitro datasets represent the true complexity and diversity of environmental samples.       5.3 Downstream analysis  5.3.1 taxonomic classification   5.3.2 differential abundance   5.3.3 ecological diversity    "],
["benchmarking-dataset.html", "6 Benchmarking Dataset", " 6 Benchmarking Dataset  Mixture Study   "],
["ecological-diversity-robustness.html", "7 Ecological Diversity Robustness 7.1 Overview", " 7 Ecological Diversity Robustness  7.1 Overview  Background Evaluating alpha and beta diversity metrics  Factors  preprocessing methods  data characteristics  sequencing error rates  primer region  read length    OTU table  depth/ coverage - sparsity  when does the diversity metric break?     "],
["gold-standard-pipeline.html", "8 Gold Standard Pipeline", " 8 Gold Standard Pipeline  Trim primers - dereplicate  Chimera check? - multiple methods orthogonal methods?, annotate don’t filter  BLAST 100% - all perfect matches  Unmatched  Multiple sequence alignment  Filter sequences with low alignment scores   phylogenetic placement   Generate OTUs for different branch distances  Keep unique singletons - based on distance from non-singleton clusters, vary similarity based on cluster size, likelihood model for whether sequencing error or biological mutation.  Rare orgs highly similar to large OTU less likely ecologically different?   Alternative taxa classification  Multiple methods  pairwise alignment  kmer  phylogenetic placement  other…   Multiple reference databases  evaluate individual sequences based on confidence - presence in multiple databases, infernal and rnammer alignment score?  supporting sequence data - whole genome, sanger, 16S PCR from isolate  likelihood of being a chimera   Compare other methods to results generated using gold standard pipeline  performance metrics     Copy number correction  based on phylogenetic placement    "],
["assessment-framework.html", "9 Assessment Framework", " 9 Assessment Framework  Outline measurement process - fishbone diagram Differentiate between biological variability and measurement error  How are these two sources of variability currently addressed in pipelines Are there any known sources of biological variability not currently being addressed  What is known about each source of variability?  uncertainty budget?  dark uncertianty - unaccounted for variability?  Robustness testing  determine largest source of bias/ uncertianty in measurement process  Schmidt 2014 Repeatability and Reproducibility Evaluation - no gold standard method, or known truth  Quantifying similarity in results  Schmidt et al 2014 Env Micro Schloss et al 2016  Method with similar assumptions Methods with different assumption   Charaterize individual measurement process steps based on robustness testing/ sensitivity analysis  benchmarking datasets  environmental  in lab prepared material - combination of known samples  in lab prepared material - combination of unknown samples  in silico - various levels of simulation based on reference and environmental data availablility   for each type of dataset summarize benefits and limitations to each  Evaluation Framework  Requirements  Benchmarking dataset  Metric for evaluating performance  Assumption Based vs. Emerical Assessment  Schloss et al. paper using within and between cluster distances to benchmark clustering methods   Thinking about what is being measured - biological question    Comparison of methods  Benchmarking  Gold standard method - known correct answer  For new method if different how to determine if new method is better?   Reference dataset - known truth   Evaluation Methods  Benchmarking  Comparing results to known truth  limitation with biological applications no real ground truth.  For physical and chemical metrology a ground truth is determined through uncertainty analysis with traceability to the SI. Often requiring the use of two orthogonal methods and uncertainty budgets. Currently no methods for traceing sequence data to an SI.    Benchmarking datasets are generated from materials or in silico with a level of confidence regarding expected results.  Discussion of top down vs. bottom up uncertainty analysis for use in establishing truth with benchmarking datasets.   Results from analysis of benchmarking datasets evaluated using performance metrics, e.g. truth tables for qualitative data.   Benchmarking- optimization  evaluating method performance when subjected to changes in either quality of data (increased noise) or use of different methods or parameters (optimization) similar to sensitivity analysis but comparing results to a dataset with known truth.      "],
["references.html", "10 References", " 10 References    Albertsen, Mads, Søren M Karst, Anja S Ziegler, Rasmus H Kirkegaard, and Per H Nielsen. 2015. “Back to Basics - The Influence of DNA Extraction and Primer Choice on Phylogenetic Analysis of Activated Sludge Communities.” PloS One 10 (7). Public Library of Science: e0132783. doi:10.1371/journal.pone.0132783.   Brooks, J Paul, David J Edwards, Michael D Harwich, Maria C Rivera, Jennifer M Fettweis, Myrna G Serrano, Robert A Reris, et al. 2015. “The truth about metagenomics: quantifying and counteracting bias in 16S rRNA studies.” BMC Microbiology 15 (1): 66. doi:10.1186/s12866-015-0351-6.   Bunge, John, Amy Willis, and Fiona Walsh. 2014. “Estimating the Number of Species in Microbial Diversity Studies.” Annual Review of Statistics and Its Application 1 (1): 427–45. doi:10.1146/annurev-statistics-022513-115654.   Chao, Anne, Nicholas J. Gotelli, T. C. Hsieh, Elizabeth L. Sander, K. H. Ma, Robert K. Colwell, and Aaron M. Ellison. 2014. “Rarefaction and extrapolation with Hill numbers: A framework for sampling and estimation in species diversity studies.” Ecological Monographs 84 (1): 45–67. doi:10.1890/13-0133.1.   Chen, Wei, Clarence K. Zhang, Yongmei Cheng, Shaowu Zhang, and Hongyu Zhao. 2013. “A Comparison of Methods for Clustering 16S rRNA Sequences into OTUs.” PLoS ONE 8 (8). doi:10.1371/journal.pone.0070837.   Chiu, Chun-Huo, and Anne Chao. 2015. “Estimating and comparing microbial diversity in the presence of sequencing errors.” PeerJ, 1–13. doi:10.7287/peerj.preprints.11.   Clooney, Adam G, Fiona Fouhy, Roy D Sleator, Aisling O’ Driscoll, Catherine Stanton, Paul D Cotter, and Marcus J Claesson. 2016. “Comparing Apples and Oranges?: Next Generation Sequencing and Its Impact on Microbiome Analysis.” PloS One 11 (2): e0148028. doi:10.1371/journal.pone.0148028.   “Comparison of two next-generation sequencing technologies for resolving highly complex microbiota composition using tandem variable 16S rRNA gene regions.” 2010. Nucleic Acids Research 38 (22). Department of Microbiology, University College Cork, Cork, Ireland. mclaesson@bioinfo.ucc.ie; Oxford University Press: e200. doi:10.1093/nar/gkq873.   D’Amore, Rosalinda, Umer Zeeshan Ijaz, Melanie Schirmer, John G Kenny, Richard Gregory, Alistair C Darby, Christopher Quince, et al. 2016. “A comprehensive benchmarking study of protocols and sequencing platforms for 16S rRNA community profiling.” BMC Genomics 17. BMC Genomics: 1–40. doi:10.1186/s12864-015-2194-9.   Ghodsi, Mohammadreza, Bo Liu, and Mihai Pop. 2011. “DNACLUST: accurate and efficient clustering of phylogenetic marker genes.” BMC Bioinformatics 12 (1). BioMed Central Ltd: 271. doi:10.1186/1471-2105-12-271.   Goodrich, Julia K., Sara C. Di Rienzi, Angela C. Poole, Omry Koren, William A. Walters, J. Gregory Caporaso, Rob Knight, and Ruth E. Ley. 2014. “Conducting a Microbiome Study.” Cell 158 (2). Elsevier Inc.: 250–62. doi:10.1016/j.cell.2014.06.037.   He, Yan, J Gregory Caporaso, Xiao-Tao Jiang, Hua-Fang Sheng, Susan M Huse, Jai Ram Rideout, Robert C Edgar, et al. 2015. “Stability of operational taxonomic units: an important but neglected property for analyzing microbial diversity.” Microbiome 3 (1). ??? 20. doi:10.1186/s40168-015-0081-x.   Huse, Susan M, David Mark Welch, Hilary G Morrison, and Mitchell L Sogin. 2010. “Ironing out the wrinkles in the rare biosphere through improved OTU clustering.” Environmental Microbiology 12 (7): 1889–98. doi:10.1111/j.1462-2920.2010.02193.x.   Jonsson, Viktor, Tobias Österlund, Olle Nerman, and Erik Kristiansson. 2016. “Statistical evaluation of methods for identification of differentially abundant genes in comparative metagenomics.” BMC Genomics 17 (1). BioMed Central: 78. doi:10.1186/s12864-016-2386-y.   Kennedy, Katherine, Michael W. Hall, Michael D J Lynch, Gabriel Moreno-Hagelsieb, and Josh D. Neufeld. 2014. “Evaluating bias of Illumina-based bacterial 16S rRNA gene profiles.” Applied and Environmental Microbiology 80 (18): 5717–22. doi:10.1128/AEM.01451-14.   Kopylova, Evguenia, Jose A Navas-molina, Céline Mercier, and Zech Xu. 2014. “Open-Source Sequence Clustering Methods Improve the State Of the Art.” MSystems 1 (1): 1–16. doi:10.1128/mSystems.00003-15.Editor.   Kozich, James J., Sarah L. Westcott, Nielson T. Baxter, Sarah K. Highlander, and Patrick D. Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” Applied and Environmental Microbiology 79 (17): 5112–20. doi:10.1128/AEM.01043-13.   Kunin, Victor, Anna Engelbrektson, Howard Ochman, and Philip Hugenholtz. 2010. “Wrinkles in the rare biosphere: pyrosequencing errors can lead to artificial inflation of diversity estimates.” Environmental Microbiology 12 (1): 118–23. doi:10.1111/j.1462-2920.2009.02051.x.   Ricotta, Carlo. 2007. “A semantic taxonomy for diversity measures.” Acta Biotheoretica 55 (1): 23–33. doi:10.1007/s10441-007-9008-7.   Schirmer, Melanie, Umer Z. Ijaz, Rosalinda D’Amore, Neil Hall, William T. Sloan, and Christopher Quince. 2015. “Insight into biases and sequencing errors for amplicon sequencing with the Illumina MiSeq platform.” Nucleic Acids Research 43 (6). doi:10.1093/nar/gku1341.   Schloss, Patrick D. 2016. “Application of database-independent approach to assess the quality of OTU picking methods.”   Schloss, Patrick D., and Sarah L. Westcott. 2011. “Assessing and Improving Methods Used in Operational Taxonomic Unit-Based Approaches for 16S rRNA Gene Sequence Analysis.” Applied and Environmental Microbiology 77 (10): 3219–26. doi:10.1128/AEM.02810-10.   Schmidt, Thomas S B, Jo??o F. Matias Rodrigues, and Christian von Mering. 2015. “Limits to robustness and reproducibility in the demarcation of operational taxonomic units.” Environmental Microbiology 17 (5): 1689–1706. doi:10.1111/1462-2920.12610.   Tuomisto, Hanna. 2012. “An updated consumer’s guide to evenness and related indices.” Oikos 121 (8): 1203–18. doi:10.1111/j.1600-0706.2011.19897.x.   Westcott, Sarah L, and Patrick D Schloss. 2015. “De novo clustering methods out-perform reference-based methods for assigning 16S rRNA gene sequences to operational taxonomic units.” PeerJ 3: e1487. doi:10.7287/peerj.preprints.1466v1.   Wu, Dongying, Ladan Doroud, and Jonathan a. Eisen. 2013. “TreeOTU Operational Taxonomic Unit Classification Based on Phylogenetic Trees.” Arxiv.Org, 23. http://arxiv.org/abs/1308.6333.   Yu, Guoqin, Doug Fadrosh, James J. Goedert, Jacques Ravel, and Alisa M. Goldstein. 2015. “Nested PCR Biases in Interpreting Microbial Community Structure in 16S rRNA Gene Sequence Datasets.” Plos One 10 (7): e0132253. doi:10.1371/journal.pone.0132253.   Zhou, Jizhong, Liyou Wu, Ye Deng, Xiaoyang Zhi, Yi-Huei Jiang, Qichao Tu, Jianping Xie, Joy D Van Nostrand, Zhili He, and Yunfeng Yang. 2011. “Reproducibility and quantitation of amplicon sequencing-based detection.” The ISME Journal 5 (8). Nature Publishing Group: 1303–13. doi:10.1038/ismej.2011.11.   "]
]
