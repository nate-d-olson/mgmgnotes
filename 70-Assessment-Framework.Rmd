# Assessment Framework
* Outline measurement process - fishbone diagram  

* Differentiate between biological variability and measurement error  
    - How are these two sources of variability currently addressed in pipelines
    - Are there any known sources of biological variability not currently being addressed  

* What is known about each source of variability?  
    - uncertainty budget?  
    - dark uncertianty - unaccounted for variability?  

* Robustness testing  
    - determine largest source of bias/ uncertianty in measurement process  
    - Schmidt 2014 Repeatability and Reproducibility
    - Evaluation - no gold standard method, or known truth
        + Quantifying similarity in results
            * Schmidt et al 2014 Env Micro
            * Schloss et al 2016
        + Method with similar assumptions
        + Methods with different assumption

* Charaterize individual measurement process steps based on robustness testing/ sensitivity analysis  
    - benchmarking datasets  
        + environmental  
        + in lab prepared material - combination of known samples  
        + in lab prepared material - combination of unknown samples  
        + in silico - various levels of simulation based on reference and environmental data availablility  
    - for each type of dataset summarize benefits and limitations to each  

* Evaluation Framework
    - Requirements  
        + Benchmarking dataset  
        + Metric for evaluating performance  
            * Assumption Based vs. Emerical Assessment  
                + Schloss et al. paper using within and between cluster distances to benchmark clustering methods  
            * Thinking about what is being measured - biological question  

* Comparison of methods  
    - Benchmarking  
        + Gold standard method - known correct answer  
            * For new method if different how to determine if new method is better?  
        + Reference dataset - known truth  
    - Evaluation Methods  
        + Benchmarking  
            * Comparing results to known truth  
                - limitation with biological applications no real ground truth.
                    + For physical and chemical metrology a ground truth is determined through uncertainty analysis with traceability to the SI. Often requiring the use of two orthogonal methods and uncertainty budgets. Currently no methods for traceing sequence data to an SI.  
            * Benchmarking datasets are generated from materials or _in silico_ with a level of confidence regarding expected results.  
                - Discussion of top down vs. bottom up uncertainty analysis for use in establishing truth with benchmarking datasets.  
            * Results from analysis of benchmarking datasets evaluated using performance metrics, e.g. truth tables for qualitative data.  
        + Benchmarking- optimization  
            * evaluating method performance when subjected to changes in either quality of data (increased noise) or use of different methods or parameters (optimization) similar to sensitivity analysis but comparing results to a dataset with known truth.  
